# Aurora Evaluation

## Overview

This repository contains scripts to evaluate Aurora and Vivace PCC on Mininet.

## Requirements

- Aurora-PCC: https://github.com/PCCproject/PCC-RL

- Aurora-RL: https://github.com/PCCproject/PCC-Uspace/tree/deep-learning

- Vivace: https://github.com/PCCproject/PCC-Uspace/tree/NSDI-2018

Follow respective instructions in each repository to build from sources.
Train RL models in Aurora-RL.

Other requirements:

- Mininet

- Iperf3

## Description and usage

The API consists of just two things: `command` and `FileData`.
A `command` encapsulates a Mininet command to run on hosts,
and `command/command.py` contains the commands to run Aurora/Vivace/iperf3
with appropriate switches and logging.
`FileData` parses log files generated by Aurora/Vivace/iperf3 into a container 
for its data.

As an overview, try:

```
python ./run_figure2.py -au /path/to/aurora-pcc -rl /path/to/aurora-rl -m /path/to/trained/aurora/model [-vi /path/to/vivace/pcc] [-l /where/to/store/logs]
```

Logs are by default stored in `./testing_logs`.

In the log directory, you should see log files whose filenames follow a 
key-value format.
The `expt` tag is the primary key to organize all log files generated for the 
same experiment, and you may find it useful to add your own tags.

Then run:

```
python plot_rate_rtt.py -d ./testing_logs
```

This parses __all__ log files in the directory and graphs them for throughput
and RTT against time.
The output is written to `<expt>.pdf` in the log directory.

These two files should illustrate usage of the API.
Be careful about changing the Aurora command in `command/command.py`;
running with wrong switches or values will fail silently and just give bad data!
